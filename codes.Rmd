---
title: "&nbsp;"
output: 
  html_document:
      css: styles.css
      toc: true
      number_sections: true
      toc_float:
        collapsed: false
        smooth_scroll: false
---
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-165847699-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-165847699-1');
</script>


<link rel="stylesheet" href="styles.css" type="text/css">

Here is some code for procedures commonly used in empirical analysis in Operations Management and Economics. The code is mostly self-contained, but requires a basic understanding of `data.table` and piping `%>%`. 

The code should be fast (due to `data.table` and other user contributed packages and not me). But because my primary focus was (and is) to write code that I can easily read months later, there are probably much better alternatives. 

Please email me if you spot any errors or have any questions or suggestions! I also take requests, so please email me if you think that there are commonly used routines that I've missed. 

```{r echo=FALSE, message=FALSE}
library(pacman)

pacman::p_load(data.table, dplyr, magrittr, stargazer, lfe, fixest, lubridate, zoo, stringr,
               janitor, purrr, MatchIt)

knitr::opts_chunk$set(tidy.opts=list(width.cutoff=60),tidy=FALSE)
knitr::opts_chunk$set(cache=TRUE)
knitr::opts_chunk$set(message=FALSE)
knitr::opts_chunk$set(warning=FALSE)
knitr::opts_chunk$set(size="footnotesize")

```

***
# WRDS
***
Because WRDS contains datasets that are very popular among OM scholars and economists, I begin with code that allows you to fetch data from WRDS in R directly, and not require you to log into WRDS each time and painstakingly choose the dataset and variables over and over again. 

You can find this tutorial on WRDS, but here is a summary and example codes. 

Setup (for OS X): 
  
  1. Install and load the package `RPostgres`. 
  2. Using terminal create the file .pgpass by entering the following in terminal: `nano ~/.pgpass`
  3. Input `wrds-pgdata.wharton.upenn.edu:9737:wrds:wrds_username:wrds_password` in the .pgpass using a text editor (e.g., Sublime Text 3).
  4. Secure this file by entering the following in terminal: `chmod 600 ~/.pgpass`

## Connect to WRDS in R

```{r crsp-compustat-fundq-hide, echo = F}
# Required Package
library(RPostgres)

# Connect to WRDS
wrds <- dbConnect(Postgres(),
                  host='wrds-pgdata.wharton.upenn.edu',
                  port=9737,
                  dbname='wrds',
                  sslmode='require',
                  user='ckwon93')
```


```{r crsp-compustat-fundq-real, eval = F, echo = T}
# Required Package
library(RPostgres)

# Connect to WRDS
wrds <- dbConnect(Postgres(),
                  host='wrds-pgdata.wharton.upenn.edu',
                  port=9737,
                  dbname='wrds',
                  sslmode='require',
                  user='USERID') # Input USER ID here. 
```

## Compustat Fundamentals Quarterly

```{r compustat-fundq-data, eval = F}

# Download data 
compustat_fundQ_DT <- dbSendQuery(wrds, 
"select CONM, TIC, EXCHG,GVKEY, DATADATE, INVTQ, ATQ, SALEQ, COGSQ  
                    from COMP.FUNDQ                                  
                    WHERE DATADATE >= '2018-03-01' AND DATADATE < '2018-04-01'")

# Fetch all rows
compustat_fundQ_DT <- dbFetch(compustat_fundQ_DT, n=-1)

```


## Compustat Fundamentals Annual

```{r compustat-funda-data, eval = F}

# Download data 
compustat_fundA_DT <- dbSendQuery(wrds, 
"select CONM, TIC, EXCHG,GVKEY, DATADATE, INVT, AT, SALE, COGS  
                    from COMP.FUNDA                                  
                    WHERE DATADATE >= '2018-01-01' AND DATADATE < '2019-04-01'")

# Fetch all rows
compustat_fundA_DT <- dbFetch(compustat_fundA_DT, n=-1)

```


## CRSP 

```{r CRSP, eval = F}

# Download data 
CRSP_daily_DT <- dbSendQuery(wrds, "SELECT CUSIP,PERMNO,DATE,PRC,SHROUT,VOL
                   FROM CRSP.DSF
                   WHERE PERMNO IN (14593) 
                   AND DATE BETWEEN '2010-01-01' and '2018-04-01'")

# Fetch all rows
CRSP_daily_DT <- dbFetch(CRSP_daily_DT, n=-1)

```




***
# Data Preliminaries
***

## Setup

Instead of loading each package manually, I use the following approach:
```{r load-packages}
# First load the package 'pacman'
library(pacman)

# Load every other package required using p_load
pacman::p_load(data.table, magrittr, stargazer, fixest, lfe, readstata13, rmatio, openxlsx)

```

This way, I dont' have to keep writing out `library(package_name)` over and over again. 

## Reading Data

We typically encounter three types of data files: 

  1. .CSV
  2. .XLSX
  3. .DAT 
  4. .MAT 

Below are the packages and codes that I use for the four types. Note that I pipe everything into setDT() to convert the data frames into a data table (except when using data table's fread function which handles this automatically).

```{r read_languages, eval = F}
# CSV
CSV_DT <- data.table::fread("/Users/ckwon/Dropbox/Academic Research/data.csv")

# DAT
dat_DT <- readstata13::read.dta13("/Users/ckwon/Dropbox/Academic Research/data.dat") %>% setDT()

# XLSX
CSV_DT <- openxlsx::read.xlsx("/Users/ckwon/Dropbox/Academic Research/data.dat") %>% setDT() 

# MAT 
MAT_DT <- read.mat("/Users/ckwon/Dropbox/Academic Research/data.dat") %>% setDT() 

```

## Winsorizing
```{r winsorizing,  eval = F}
# Required package
pacman::p_load(DescTools)

# Choose variables to winsorize
winz_var <- c("at", "grossmargin", "debt_equity", "roe")

# Winsorize (2% and 98%)
DT[,(winz_var) := lapply(.SD, Winsorize, minval = NULL, maxval = NULL,  probs = c(0.02, 0.98),  na.rm = TRUE), .SDcols = winz_var]
```

## Trimming

People use 'trimming' to mean two different things: (1) Removing observations that fall above or below a certain quantile, or (2) removing observations that don't fit a certain criterion, e.g., return on equity is smaller than 0.01. I present code for both meanings below. 

```{r trimming, eval = F}

# Definition 1: Delete obs. where return on equity falls outside the (2%, 98%) percentiles
quantile_roe_98 <- quantile(DT$roe, probs = 0.98)
quantile_roe_02 <- quantile(DT$roe, probs = 0.02)

DT <- DT %>%
  .[!(roe > quantile_roe_98)] %>%  # Delete observations that fall 
  
  .[!(roe < quantile_roe_02)]



# Definition 2: Separately 
DT <- DT %>%
  .[roe > 0.01] %>%    # Only choose samples where return on equity is greater than 0.01
  .[!(at == 0)] %>%    # Drop obs that have assets equal to 0
  .[invt < 500] %>%    # Drop obs that have inventories smaller than 500
  .[!is.na(cogs)]      # Drop obs that have missing cost of goods sold values

# Definition 2: Altogether: I prefer the method above, as it is easier to read. 
DT <- DT %>%
  .[roe > 0.01 & !(at ==0) & invt < 500 & !(is.na(cogs))]
```


***
# Regressions
***
Now, onto the fun part!There are many packages that run regressions very efficiently in R. My two favorites are the packages `lfe` and `fixest`. I currently favor the former, as the latter is not compatible with `stargazer`. However, `fixest` is often faster and includes a very fast Poisson fixed effects procedure which `lfe` does not have. 

## Ordinary Least Squares

```{r OLS}
# Initialize dataset (should already be in R)
DT <- iris %>% setDT()

# Run regression
reg_Sepal.Length_Sepal.Width <- felm(Sepal.Length ~ Sepal.Width, data = DT)

```

Consistent naming conventions are important when you run many regressions and need to later export them into $LaTeX$. I follow the convention of starting with reg and then appending in the name of the dependent variable and the most independent variable of interest, e.g. `reg_DEPVAR_INDEPVAR`. 

## Poisson 

## Zero Inflated Poisson 



# Fixed Effects

# Difference in Differences

# Regression Discontinuity

# Instrumental Variables 

# Presenting Results
***
